---
---

@article{shen2025simultaneous,
  title={Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards},
  author={Shen, Yiran and Xia, Yu and Chang, Jonathan and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2510.01167},
  year={2025},
  field = {NeurIPS MATH-AI Workshop},
}

@inproceedings{xia-etal-2025-sand,
    title = "{SAND}: Boosting {LLM} Agents with Self-Taught Action Deliberation",
    author = "Xia, Yu  and
      Shen, Yiran Jenny  and
      Wu, Junda  and
      Yu, Tong  and
      Kim, Sungchul  and
      Rossi, Ryan A.  and
      Yao, Lina  and
      McAuley, Julian",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.152/",
    doi = "10.18653/v1/2025.emnlp-main.152",
    pages = "3062--3077",
    ISBN = "979-8-89176-332-6",
    abstract = "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20{\%} improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches."
}
